import os
import numpy as np
import cv2
import tensorflow as tf
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import Dense, Conv2D, MaxPool2D, Flatten, Dropout
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input
from tensorflow.python.framework.ops import disable_eager_execution
from tensorflow.keras.optimizers import Adam, SGD
from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint
from tensorflow.keras.regularizers import l2
import random
disable_eager_execution()

### scaling and random_crop def
def random_crop(img, random_crop_size):
    # Note: image_data_format is 'channel_last'
    assert img.shape[2] == 3
    #apply scaling
    size = random.randint(256, 512)
    img = cv2.resize(img, (size, size))

    ### apply random crop
    height, width = img.shape[0], img.shape[1]
    dy, dx = random_crop_size
    x = np.random.randint(0, width - dx + 1)
    y = np.random.randint(0, height - dy + 1)

    return img[y:(y+dy), x:(x+dx), :]

### scaling and random_crop generator
def crop_generator(batches, crop_length):
    """Take as input a Keras ImageGen (Iterator) and generate random
    crops from the image batches generated by the original iterator.
    """
    while True:
        batch_x, batch_y = next(batches)
        batch_crops = np.zeros((batch_x.shape[0], crop_length, crop_length, 3))
        for i in range(batch_x.shape[0]):
            batch_crops[i] = random_crop(batch_x[i], (crop_length, crop_length))
        yield (batch_crops, batch_y)

### set dirs
train_path = "ds/train/"
val_path = "ds/val/"

### main variables
Batch_size = 64
Learning_rate = 0.01
Momentum = 0.9
Epochs = 100 

### defining generators
trdata = ImageDataGenerator(preprocessing_function=preprocess_input, horizontal_flip=True, channel_shift_range=100)
traindata = trdata.flow_from_directory(directory=train_path,target_size=(256,256), batch_size=Batch_size)
train_crops = crop_generator(traindata, 224)

val_data = ImageDataGenerator(preprocessing_function=preprocess_input)
validation_data = val_data.flow_from_directory(directory=val_path, target_size=(224,224), batch_size=Batch_size)


### convert to tf.data
relevant_batch_size = None
ds_train = tf.data.Dataset.from_generator(lambda:
    train_crops,
    output_types=(tf.float64, tf.float32),
    output_shapes=([relevant_batch_size, 224, 224, 3], [relevant_batch_size, 2])
).shuffle(64)

ds_val = tf.data.Dataset.from_generator(lambda:
    validation_data,
    output_types=(tf.float64, tf.float32),
    output_shapes=([relevant_batch_size, 224, 224, 3], [relevant_batch_size, 2])
).shuffle(64)

### setting Xavier initializer
initializer = tf.keras.initializers.GlorotNormal()

# model architecture
model = Sequential()
model.add(Conv2D(input_shape=(224,224,3),filters=64,kernel_size=(3,3),padding="same", activation="relu", kernel_initializer=initializer, kernel_regularizer=l2(5e-4)))
model.add(Conv2D(filters=64,kernel_size=(3,3),padding="same", activation="relu", kernel_initializer=initializer, kernel_regularizer=l2(5e-4)))
model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))
model.add(Conv2D(filters=128, kernel_size=(3,3), padding="same", activation="relu", kernel_initializer=initializer, kernel_regularizer=l2(5e-4)))
model.add(Conv2D(filters=128, kernel_size=(3,3), padding="same", activation="relu", kernel_initializer=initializer, kernel_regularizer=l2(5e-4)))
model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))
model.add(Conv2D(filters=256, kernel_size=(3,3), padding="same", activation="relu", kernel_initializer=initializer, kernel_regularizer=l2(5e-4)))
model.add(Conv2D(filters=256, kernel_size=(3,3), padding="same", activation="relu", kernel_initializer=initializer, kernel_regularizer=l2(5e-4)))
model.add(Conv2D(filters=256, kernel_size=(3,3), padding="same", activation="relu", kernel_initializer=initializer, kernel_regularizer=l2(5e-4)))
model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))
model.add(Conv2D(filters=512, kernel_size=(3,3), padding="same", activation="relu", kernel_initializer=initializer, kernel_regularizer=l2(5e-4)))
model.add(Conv2D(filters=512, kernel_size=(3,3), padding="same", activation="relu", kernel_initializer=initializer, kernel_regularizer=l2(5e-4)))
model.add(Conv2D(filters=512, kernel_size=(3,3), padding="same", activation="relu", kernel_initializer=initializer, kernel_regularizer=l2(5e-4)))
model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))
model.add(Conv2D(filters=512, kernel_size=(3,3), padding="same", activation="relu", kernel_initializer=initializer, kernel_regularizer=l2(5e-4)))
model.add(Conv2D(filters=512, kernel_size=(3,3), padding="same", activation="relu", kernel_initializer=initializer, kernel_regularizer=l2(5e-4)))
model.add(Conv2D(filters=512, kernel_size=(3,3), padding="same", activation="relu", kernel_initializer=initializer, kernel_regularizer=l2(5e-4)))
model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))
model.add(Flatten())
model.add(Dense(units=4096,activation="relu", kernel_initializer=initializer, kernel_regularizer=l2(5e-4)))
model.add(Dropout(0.5))
model.add(Dense(units=4096,activation="relu", kernel_initializer=initializer, kernel_regularizer=l2(5e-4)))
model.add(Dropout(0.5))
model.add(Dense(units=1001, activation="softmax", kernel_initializer=initializer, kernel_regularizer=l2(5e-4)))

### defining optimizer and compiler
opt = SGD(learning_rate=Learning_rate, momentum=Momentum)
model.compile(optimizer=opt, loss=tf.keras.losses.categorical_crossentropy, metrics=['accuracy'])

### setting callbacks
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=1, verbose=1)
model_checkpoint_callback1 = ModelCheckpoint(
    filepath='./model/best_checkpoint',
    save_weights_only=False,
    monitor='val_accuracy',
    mode='max',
    save_best_only=True)

model_checkpoint_callback2 = ModelCheckpoint(
    filepath='./model/best_checkpoint.h5',
    save_weights_only=True,
    monitor='val_accuracy',
    mode='max',
    save_best_only=True)

filename='log.csv'
history_logger=tf.keras.callbacks.CSVLogger(filename, separator=",", append=True)

### training procedure
history = model.fit(train_crops, validation_data=validation_data, epochs=Epochs, steps_per_epoch=int(traindata.samples/Batch_size), validation_steps=int(validation_data.samples/Batch_size), 
					callbacks=[reduce_lr, model_checkpoint_callback2, model_checkpoint_callback1, history_logger])